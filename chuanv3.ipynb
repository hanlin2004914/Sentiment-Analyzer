{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpEJefdzPpgP",
        "outputId": "0342d377-1868-4c2b-d3f3-cca17b4bb820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyarrow: 19.0.1\n",
            "datasets: 4.3.0\n"
          ]
        }
      ],
      "source": [
        "# Remove RAPIDS bits that force old pyarrow\n",
        "!pip -q uninstall -y cudf-cu12 pylibcudf-cu12\n",
        "\n",
        "# Install compatible, up-to-date stack\n",
        "!pip -q install -U \"pyarrow>=21,<23\" datasets openai scikit-learn tqdm huggingface_hub\n",
        "\n",
        "# Sanity check\n",
        "import pyarrow, datasets, sklearn, openai, huggingface_hub\n",
        "print(\"pyarrow:\", pyarrow.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "DEEPSEEK_BASE_URL = \"https://api.deepseek.com\"\n",
        "DEEPSEEK_MODEL = \"deepseek-chat\"   # or \"deepseek-chat\" on some accounts\n",
        "\n",
        "# Prompt once if not set\n",
        "if not os.getenv(\"DEEPSEEK_API_KEY\") or os.getenv(\"DEEPSEEK_API_KEY\") == \"YOUR_DEEPSEEK_API_KEY\":\n",
        "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Paste DEEPSEEK_API_KEY (input hidden): \").strip()\n",
        "\n",
        "# quick sanity ping (tiny request)\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=DEEPSEEK_BASE_URL)\n",
        "resp = client.chat.completions.create(\n",
        "    model=DEEPSEEK_MODEL,\n",
        "    messages=[{\"role\":\"system\",\"content\":\"Say OK.\"},{\"role\":\"user\",\"content\":\"OK?\"}],\n",
        "    max_tokens=1, temperature=0\n",
        ")\n",
        "print(\"DeepSeek ready:\", resp.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5DsvlANRZC3",
        "outputId": "ad03e505-d9c6-45de-f824-cea4b851185b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek ready: 好的\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "# login()  # <- uncomment, run once if you need gated dataset access\n"
      ],
      "metadata": {
        "id": "Sqy6TpivTGj6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Paste HF token (hidden): \").strip()\n",
        "# set both common env names so libraries can find it\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "login(token=os.environ[\"HF_TOKEN\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wafk3NwFTKmO",
        "outputId": "9e1c7772-5dfb-4edc-c7be-2978cef23b95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste HF token (hidden): ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset loader (self-contained, fixes NameError)\n",
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"TheFinAI/en-fpb\"\n",
        "SPLIT = \"train\"\n",
        "\n",
        "# Define defaults here so this cell works even if a prior config cell was skipped\n",
        "SEED = 42\n",
        "N_SAMPLES = 100\n",
        "\n",
        "ds = load_dataset(DATASET_NAME, split=SPLIT)\n",
        "\n",
        "# Use \"is not None\" so 0 doesn't accidentally mean \"skip sampling\"\n",
        "if N_SAMPLES is not None:\n",
        "    ds = ds.shuffle(seed=SEED).select(range(min(N_SAMPLES, len(ds))))\n",
        "\n",
        "print(\"Rows:\", len(ds))\n",
        "ds[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2T5SW5OTtKs",
        "outputId": "a1a2ed1a-de19-457d-b815-9f8dc1a5fd75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'fpb2204',\n",
              " 'query': 'Analyze the sentiment of this statement extracted from a financial news article. Provide your answer as either negative, positive, or neutral.\\nText: In August , Latvijas Finieris ordered all production lines for a new green veneer mill to be built in Ukmerge , Lithuania .\\nAnswer:',\n",
              " 'answer': 'neutral',\n",
              " 'text': 'In August , Latvijas Finieris ordered all production lines for a new green veneer mill to be built in Ukmerge , Lithuania .',\n",
              " 'choices': ['positive', 'neutral', 'negative'],\n",
              " 'gold': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALLOWED = [\"positive\", \"neutral\", \"negative\"]\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a finance sentiment rater. \"\n",
        "    \"Return exactly one word from this set: positive, neutral, negative. \"\n",
        "    \"No explanation. If uncertain, choose neutral.\"\n",
        ")\n",
        "\n",
        "def build_user_prompt(row):\n",
        "    # The dataset already includes an instruction in 'query' and a financial sentence in 'text'.\n",
        "    # We’ll be explicit and pass the sentence and the allowed labels.\n",
        "    text = row[\"text\"]\n",
        "    return (\n",
        "        \"Classify the sentiment of the following financial news statement as \"\n",
        "        \"one of: positive, neutral, negative.\\n\\n\"\n",
        "        f\"Statement: {text}\\n\\n\"\n",
        "        \"Answer with only one word.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Te-FzlLuTyIF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, json, re, pathlib, random\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI, APIError, RateLimitError, APITimeoutError\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=DEEPSEEK_BASE_URL)\n",
        "\n",
        "CACHE_PATH = pathlib.Path(\"deepseek_en_fpb_predictions.jsonl\")\n",
        "\n",
        "def normalize_label(s: str) -> str:\n",
        "    if not s: return \"neutral\"\n",
        "    s = s.strip().lower()\n",
        "    # accept the first allowed label mentioned\n",
        "    for lab in ALLOWED:\n",
        "        if re.search(rf\"\\b{lab}\\b\", s):\n",
        "            return lab\n",
        "    # sometimes models return punctuation/newline; take a single word guess\n",
        "    s = re.sub(r\"[^a-z]\", \" \", s).split()\n",
        "    for tok in s:\n",
        "        if tok in ALLOWED:\n",
        "            return tok\n",
        "    return \"neutral\"\n",
        "\n",
        "def chat_once(prompt, temperature=0.0):\n",
        "    # One call with robust error handling\n",
        "    tries, max_tries, backoff = 0, 6, 1.5\n",
        "    while True:\n",
        "        tries += 1\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=DEEPSEEK_MODEL,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\",   \"content\": prompt},\n",
        "                ],\n",
        "            )\n",
        "            return resp.choices[0].message.content\n",
        "        except (RateLimitError, APITimeoutError, APIError) as e:\n",
        "            if tries >= max_tries:\n",
        "                raise\n",
        "            time.sleep(backoff)\n",
        "            backoff *= 1.8\n",
        "\n",
        "# Resume support\n",
        "seen_ids = set()\n",
        "pred_rows = []\n",
        "if CACHE_PATH.exists():\n",
        "    with open(CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            seen_ids.add(obj[\"id\"])\n",
        "            pred_rows.append(obj)\n",
        "\n",
        "# Run inference\n",
        "for row in tqdm(ds, total=len(ds)):\n",
        "    rid = row[\"id\"]\n",
        "    if rid in seen_ids:\n",
        "        continue\n",
        "    prompt = build_user_prompt(row)\n",
        "    raw = chat_once(prompt, temperature=0.0)\n",
        "    pred = normalize_label(raw)\n",
        "    out = {\n",
        "        \"id\": rid,\n",
        "        \"text\": row[\"text\"],\n",
        "        \"gold\": row[\"answer\"],     # gold labels are strings in this dataset\n",
        "        \"pred\": pred,\n",
        "        \"raw\": raw,\n",
        "    }\n",
        "    pred_rows.append(out)\n",
        "    with open(CACHE_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "len(pred_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVVaMWy3UY_l",
        "outputId": "baa848f4-9cf5-4fd6-982d-a239760cb30f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:50<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy and macro-F1 from saved predictions or in-memory df\n",
        "import pandas as pd, pathlib, json\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def load_gold_pred():\n",
        "    if 'df' in globals() and {'gold','pred'}.issubset(df.columns):\n",
        "        return df[['gold','pred']].copy()\n",
        "    p_csv   = pathlib.Path(\"deepseek_en_fpb_predictions.csv\")\n",
        "    p_jsonl = pathlib.Path(\"deepseek_en_fpb_predictions.jsonl\")\n",
        "    if p_csv.exists():\n",
        "        d = pd.read_csv(p_csv)\n",
        "    elif p_jsonl.exists():\n",
        "        d = pd.read_json(p_jsonl, lines=True)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No predictions found. Run the evaluation cell first to create predictions.\")\n",
        "    return d[['gold','pred']].copy()\n",
        "\n",
        "d = load_gold_pred()\n",
        "y_true = d['gold'].str.lower()\n",
        "y_pred = d['pred'].str.lower()\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Macro F1 : {macro_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk26gvd-VLv4",
        "outputId": "0bc0f8eb-7e17-4b1b-9f65-bc7d98944728"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.8700\n",
            "Macro F1 : 0.8729\n"
          ]
        }
      ]
    }
  ]
}