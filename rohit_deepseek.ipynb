{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c815ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openai>=1.40.0\" \"httpx>=0.27.2\" \"datasets>=2.19.0\" \"huggingface_hub>=0.24\" \"tqdm>=4.66.4\" \"pandas>=2.2.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0795ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DeepSeek (OpenAI-compatible) + paths\n",
    "import os, getpass, json, time, platform, re\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "def ver(p):\n",
    "    try: from importlib.metadata import version; return version(p)\n",
    "    except Exception: return None\n",
    "\n",
    "BASE_URL = os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\n",
    "MODEL    = os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-v3\")\n",
    "api_key  = os.getenv(\"DEEPSEEK_API_KEY\") or os.getenv(\"API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass(\"Paste your DeepSeek API key: \")\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = api_key\n",
    "\n",
    "run_dir  = \"./results_deepseekv3\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "pred_path = os.path.join(run_dir, \"predictions.csv\")\n",
    "meta_path = os.path.join(run_dir, \"meta.json\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": MODEL,\n",
    "    \"base_url\": BASE_URL,\n",
    "    \"python\": platform.python_version(),\n",
    "    \"time_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
    "    \"openai_pkg\": ver(\"openai\"),\n",
    "    \"httpx\": ver(\"httpx\"),\n",
    "    \"datasets\": ver(\"datasets\"),\n",
    "}\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"MODEL:\", MODEL)\n",
    "print(\"BASE_URL:\", BASE_URL)\n",
    "print(\"pred_path ->\", pred_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85814425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FPB test split (TheFinAI/flare-fpb) and normalize labels\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "LABELS = [\"negative\",\"neutral\",\"positive\"]\n",
    "_alias = {\"pos\":\"positive\",\"neg\":\"negative\",\"neu\":\"neutral\",\n",
    "          \"bullish\":\"positive\",\"bearish\":\"negative\"}\n",
    "\n",
    "ds_raw = load_dataset(\"TheFinAI/flare-fpb\", split=\"test\")\n",
    "print(\"Loaded flare-fpb test:\", len(ds_raw), \"columns:\", ds_raw.column_names)\n",
    "\n",
    "# Try to locate the text and label fields heuristically\n",
    "text_col_candidates = [\"text\",\"sentence\",\"content\",\"tweet\",\"headline\"]\n",
    "label_col_candidates = [\"label\",\"labels\",\"sentiment\",\"target\"]\n",
    "cols = ds_raw.column_names\n",
    "\n",
    "text_col = next((c for c in text_col_candidates if c in cols), None)\n",
    "label_col = next((c for c in label_col_candidates if c in cols), None)\n",
    "if text_col is None:\n",
    "    # default to the first string-typed column\n",
    "    for c in cols:\n",
    "        if isinstance(ds_raw[0][c], str):\n",
    "            text_col = c; break\n",
    "if label_col is None:\n",
    "    # default to a column named 'label' like mapping; else throw\n",
    "    label_col = \"label\" if \"label\" in cols else cols[-1]\n",
    "\n",
    "print(\"Using text_col:\", text_col, \"| label_col:\", label_col)\n",
    "\n",
    "def norm_label(x):\n",
    "    v = str(x).strip().lower()\n",
    "    return _alias.get(v, v)\n",
    "\n",
    "# Build a pandas frame for easy iteration\n",
    "df = pd.DataFrame({ \"row_idx\": range(len(ds_raw)),\n",
    "                    \"text\": [ds_raw[i][text_col] for i in range(len(ds_raw))],\n",
    "                    \"label\": [norm_label(ds_raw[i][label_col]) for i in range(len(ds_raw))]})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Inference with DeepSeek using OpenAI-compatible Chat Completions\n",
    "            from openai import OpenAI\n",
    "            from tqdm import tqdm\n",
    "            import pandas as pd, time\n",
    "\n",
    "            client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=BASE_URL)\n",
    "\n",
    "            def classify(txt: str) -> str:\n",
    "                prompt = f\"\"\"Classify the sentiment of the following financial text\n",
    "as exactly one of: negative, neutral, positive.\n",
    "Return ONLY the single word label.\n",
    "\n",
    "Text:\n",
    "{txt}\n",
    "\"\"\"\n",
    "                msgs = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a careful, deterministic classifier.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                try:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=msgs,\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=5,\n",
    "                    )\n",
    "                    out = resp.choices[0].message.content.strip().lower()\n",
    "                except Exception as e:\n",
    "                    # Attempt common fallbacks if the given model is unavailable\n",
    "                    for alt in [\"deepseek-v3\",\"deepseek-chat\",\"deepseek-reasoner\"]:\n",
    "                        try:\n",
    "                            resp = client.chat.completions.create(\n",
    "                                model=alt,\n",
    "                                messages=msgs,\n",
    "                                temperature=0.0,\n",
    "                                max_tokens=5,\n",
    "                            )\n",
    "                            out = resp.choices[0].message.content.strip().lower()\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            out = f\"UNKNOWN\"\n",
    "                    else:\n",
    "                        out = \"UNKNOWN\"\n",
    "\n",
    "                # Clean to expected vocabulary\n",
    "                out = out.replace(\".\", \"\").replace(\"'\", \"\").strip()\n",
    "                if out not in {\"negative\",\"neutral\",\"positive\"}:\n",
    "                    # Try to map variants like 'pos', 'neg', etc.\n",
    "                    aliases = {\"pos\":\"positive\",\"neg\":\"negative\",\"neu\":\"neutral\"}\n",
    "                    out = aliases.get(out, \"UNKNOWN\")\n",
    "                return out\n",
    "\n",
    "            preds = []\n",
    "            for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring\"):\n",
    "                pred = classify(row[\"text\"])\n",
    "                preds.append({\"row_idx\": int(row[\"row_idx\"]), \"label\": row[\"label\"], \"pred\": pred})\n",
    "\n",
    "            pred_df = pd.DataFrame(preds)\n",
    "            pred_df.to_csv(pred_path, index=False)\n",
    "            print(\"Saved predictions to:\", pred_path)\n",
    "            pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1 (macro) and Accuracy, mirroring the user's evaluation cell\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load predictions\n",
    "df = pd.read_csv(pred_path).sort_values(\"row_idx\").drop_duplicates(\"row_idx\", keep=\"last\")\n",
    "ok = df[df[\"pred\"] != \"UNKNOWN\"].copy()\n",
    "\n",
    "# Ensure LABELS are defined\n",
    "try:\n",
    "    LABELS\n",
    "except NameError:\n",
    "    LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# --- Clean & normalize labels/predictions ---\n",
    "def clean_labels(df, col):\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "    # Map numeric labels to string equivalents\n",
    "    df[col] = df[col].replace({\n",
    "        \"0\": \"negative\", \"1\": \"neutral\", \"2\": \"positive\",\n",
    "        \"neg\": \"negative\", \"neu\": \"neutral\", \"pos\": \"positive\"\n",
    "    })\n",
    "    return df\n",
    "\n",
    "ok = clean_labels(ok, \"label\")\n",
    "ok = clean_labels(ok, \"pred\")\n",
    "\n",
    "# Keep only valid labels\n",
    "ok = ok[ok[\"label\"].isin(LABELS) & ok[\"pred\"].isin(LABELS)]\n",
    "\n",
    "# --- Compute metrics ---\n",
    "f1ma = f1_score(ok[\"label\"], ok[\"pred\"], labels=LABELS, average=\"macro\", zero_division=0)\n",
    "acc  = accuracy_score(ok[\"label\"], ok[\"pred\"])\n",
    "\n",
    "print(f\"F1: {f1ma:.4f}, Accuracy: {acc:.4f}, kept {len(ok)}/{len(df)} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
