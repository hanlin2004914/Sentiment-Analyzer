{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G1YPHSP2F8gU"
      },
      "outputs": [],
      "source": [
        "%pip -q install --upgrade --force-reinstall \\\n",
        "  openai>=1.30.0 datasets>=2.19.0 scikit-learn>=1.4.2 tqdm>=4.66.4 huggingface_hub>=0.24.0 \\\n",
        "  pandas==2.2.2 numpy==2.0.2 \"pyarrow<20\" requests==2.32.4 \"pydantic<2.12\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your DeepSeek API key securely\n",
        "if not os.getenv(\"DEEPSEEK_API_KEY\"):\n",
        "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"DeepSeek API key: \")\n",
        "\n",
        "# DeepSeek uses an OpenAI-compatible endpoint\n",
        "client = OpenAI(api_key=os.environ[\"DEEPSEEK_API_KEY\"], base_url=\"https://api.deepseek.com\")\n",
        "MODEL_NAME = \"deepseek-reasoner\"   # DeepSeek-R1 reasoning model\n",
        "print(\"Client ready ✅  Model:\", MODEL_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvVQAYoeH0cK",
        "outputId": "9e9ff590-e9ab-426f-c74f-b580550b2bcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek API key: ··········\n",
            "Client ready ✅  Model: deepseek-reasoner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify HF token has gated access, then load en-fpb\n",
        "import os, getpass\n",
        "from huggingface_hub import HfApi, HfFolder, notebook_login\n",
        "from datasets import load_dataset\n",
        "\n",
        "token = HfFolder.get_token()\n",
        "if not token:\n",
        "    notebook_login()\n",
        "    token = HfFolder.get_token()\n",
        "if not token:\n",
        "    token = os.getenv(\"HF_TOKEN\") or getpass.getpass(\"Paste HF token (hf_...): \")\n",
        "\n",
        "api = HfApi()\n",
        "print(\"HF user:\", api.whoami(token=token).get(\"name\"))\n",
        "\n",
        "\n",
        "info = api.dataset_info(\"TheFinAI/en-fpb\", token=token)\n",
        "print(\"Access OK. SHA:\", info.sha[:8], \"...\")\n",
        "\n",
        "\n",
        "ds_all = load_dataset(\"TheFinAI/en-fpb\", token=token)\n",
        "SPLIT = next((s for s in [\"test\",\"validation\",\"valid\",\"train\"] if s in ds_all), list(ds_all.keys())[0])\n",
        "ds = ds_all[SPLIT]\n",
        "print(f\"Loaded split: {SPLIT} (n={len(ds)})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B122m2_sH4Re",
        "outputId": "caa6dd8e-26af-47fe-f2b0-32442039c9d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF user: Hanlin0914\n",
            "Access OK. SHA: 98f0a91c ...\n",
            "Loaded split: test (n=970)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import json, re\n",
        "\n",
        "def row_text(row):\n",
        "    q = (row.get(\"query\") or \"\").strip()\n",
        "    t = (row.get(\"text\") or \"\").strip()\n",
        "    return f\"{q}\\n\\nText:\\n{t}\" if (q and t) else (q or t)\n",
        "\n",
        "def normalize_choices(raw):\n",
        "    out = []\n",
        "    if isinstance(raw, list):\n",
        "        for x in raw:\n",
        "            if isinstance(x, str):\n",
        "                s = x.strip()\n",
        "            elif isinstance(x, dict):\n",
        "                s = str(x.get(\"label\") or x.get(\"text\") or x.get(\"value\") or x.get(\"choice\") or \"\").strip()\n",
        "            else:\n",
        "                s = str(x).strip()\n",
        "            if s: out.append(s)\n",
        "    return out\n",
        "\n",
        "def gold_to_label(row, choices: List[str]):\n",
        "    g = row.get(\"gold\", None) if row.get(\"gold\", None) is not None else row.get(\"answer\", None)\n",
        "    if isinstance(g, int):\n",
        "        return choices[g] if 0 <= g < len(choices) else str(g)\n",
        "    if isinstance(g, str):\n",
        "        for c in choices:\n",
        "            if g.strip().lower() == c.lower():\n",
        "                return c\n",
        "        return g.strip()\n",
        "    return str(g)\n"
      ],
      "metadata": {
        "id": "eI5DprhJId6R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, random\n",
        "from openai import APIError, APIConnectionError, RateLimitError\n",
        "\n",
        "MAX_RETRIES = 6\n",
        "QPS_DELAY   = 1.0  # seconds\n",
        "\n",
        "def call_deepseek_json(text: str, labels: List[str]) -> Tuple[str, float, dict]:\n",
        "    \"\"\"Return (label, confidence, obj). Tries to parse JSON; falls back by token match.\"\"\"\n",
        "    sys = \"Return only valid JSON. Choose exactly one label from the provided set.\"\n",
        "    user = f\"\"\"Choose ONE label from this set:\n",
        "{labels}\n",
        "\n",
        "Return ONLY this JSON (no prose):\n",
        "{{\"label\": \"<one of {labels}>\", \"confidence\": <0..1>}}\n",
        "\n",
        "Text:\n",
        "{text}\"\"\"\n",
        "\n",
        "    last = None\n",
        "    for k in range(MAX_RETRIES):\n",
        "        try:\n",
        "            r = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                temperature=0,\n",
        "                messages=[{\"role\":\"system\",\"content\":sys},\n",
        "                          {\"role\":\"user\",\"content\":user}],\n",
        "            )\n",
        "            raw = r.choices[0].message.content.strip()\n",
        "            try:\n",
        "                obj = json.loads(raw)\n",
        "                lab = obj.get(\"label\"); conf = float(obj.get(\"confidence\", 0.0))\n",
        "                for L in labels:\n",
        "                    if str(lab).lower() == L.lower(): lab = L\n",
        "                if lab not in labels: lab = labels[0]\n",
        "                return lab, conf, obj\n",
        "            except Exception:\n",
        "                low = raw.lower()\n",
        "                for L in labels:\n",
        "                    if re.search(rf\"\\b{re.escape(L.lower())}\\b\", low):\n",
        "                        return L, 0.0, {\"label\": L, \"raw\": raw}\n",
        "                return labels[0], 0.0, {\"label\": labels[0], \"raw\": raw}\n",
        "        except (RateLimitError, APIConnectionError, APIError) as e:\n",
        "            last = e\n",
        "            time.sleep((2**k) + random.random())\n",
        "    raise last or RuntimeError(\"DeepSeek call failed\")\n"
      ],
      "metadata": {
        "id": "-uEnW2RXImX2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math, json, re, time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "\n",
        "BATCH_SIZE   = 8\n",
        "MAX_SAMPLES  = 0\n",
        "idx = list(range(len(ds)))[: (MAX_SAMPLES or len(ds))]\n",
        "\n",
        "def batch_prompt(batch):\n",
        "    \"\"\"\n",
        "    batch = [{\"id\": i, \"text\": ..., \"choices\": [...]}, ...]\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    for it in batch:\n",
        "        items.append({\n",
        "            \"id\": it[\"id\"],\n",
        "            \"choices\": it[\"choices\"],\n",
        "            \"text\": it[\"text\"]\n",
        "        })\n",
        "    return (\n",
        "        \"For each item, choose EXACTLY one label from that item's choices.\\n\"\n",
        "        \"Return ONLY a JSON array of objects: [{\\\"id\\\": <int>, \\\"label\\\": <string>}].\\n\\n\"\n",
        "        f\"Items:\\n{json.dumps(items, ensure_ascii=False)}\"\n",
        "    )\n",
        "\n",
        "def call_batch(batch):\n",
        "    # Works with OpenAI/xAI/DeepSeek chat-completions\n",
        "    sys = \"Return ONLY valid JSON (no prose).\"\n",
        "    user = batch_prompt(batch)\n",
        "    r = client.chat.completions.create(\n",
        "        model=MODEL_NAME, temperature=0,\n",
        "        messages=[{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}]\n",
        "    )\n",
        "    raw = r.choices[0].message.content.strip()\n",
        "    try:\n",
        "        arr = json.loads(raw)\n",
        "        # {id, label}\n",
        "        out = { int(x[\"id\"]): str(x[\"label\"]) for x in arr if \"id\" in x and \"label\" in x }\n",
        "        # case-insensitive label alignment\n",
        "        for it in batch:\n",
        "            cset = it[\"choices\"]\n",
        "            lab = out.get(it[\"id\"], cset[0])\n",
        "            for C in cset:\n",
        "                if lab.lower() == C.lower():\n",
        "                    out[it[\"id\"]] = C; break\n",
        "        return out, raw\n",
        "    except Exception:\n",
        "        # salvage: match by token per item\n",
        "        out = {}\n",
        "        for it in batch:\n",
        "            low = raw.lower()\n",
        "            guess = next((C for C in it[\"choices\"] if re.search(rf\"\\b{re.escape(C.lower())}\\b\", low)), it[\"choices\"][0])\n",
        "            out[it[\"id\"]] = guess\n",
        "        return out, raw\n",
        "\n",
        "# build batches\n",
        "examples = []\n",
        "for i in idx:\n",
        "    row = ds[i]\n",
        "    choices = normalize_choices(row.get(\"choices\")) or [\"negative\",\"neutral\",\"positive\"]\n",
        "    examples.append({\"id\": i, \"text\": row_text(row), \"choices\": choices, \"gold\": gold_to_label(row, choices)})\n",
        "\n",
        "rows = []\n",
        "for b in tqdm(range(0, len(examples), BATCH_SIZE), desc=\"Micro-batching\"):\n",
        "    batch = examples[b:b+BATCH_SIZE]\n",
        "    preds, raw = call_batch(batch)\n",
        "    for it in batch:\n",
        "        rows.append({\"idx\": it[\"id\"], \"text\": it[\"text\"], \"gold\": it[\"gold\"], \"pred\": preds[it[\"id\"]]})\n",
        "\n",
        "# metrics\n",
        "df = pd.DataFrame(sorted(rows, key=lambda r: r[\"idx\"]))\n",
        "label_set = list(OrderedDict.fromkeys(\n",
        "    (df[\"gold\"].tolist() + df[\"pred\"].tolist())\n",
        "))\n",
        "df_ok = df[df[\"pred\"].isin(label_set)]\n",
        "if len(df_ok):\n",
        "    P,R,F1,S = precision_recall_fscore_support(df_ok[\"gold\"], df_ok[\"pred\"], labels=label_set, zero_division=0)\n",
        "    acc = accuracy_score(df_ok[\"gold\"], df_ok[\"pred\"])\n",
        "    f1_micro = f1_score(df_ok[\"gold\"], df_ok[\"pred\"], average=\"micro\")\n",
        "    f1_macro = f1_score(df_ok[\"gold\"], df_ok[\"pred\"], average=\"macro\")\n",
        "    print({\"n\":len(df_ok),\"acc\":round(acc,4),\"f1_micro\":round(f1_micro,4),\"f1_macro\":round(f1_macro,4)})\n",
        "else:\n",
        "    print(\"No valid predictions (check parsing).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycB37c1fInzd",
        "outputId": "83cada0b-85df-4a5f-ea8d-6c8e7660dcc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Micro-batching: 100%|██████████| 122/122 [1:10:31<00:00, 34.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n': 970, 'acc': 0.7165, 'f1_micro': 0.7165, 'f1_macro': 0.7448}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}